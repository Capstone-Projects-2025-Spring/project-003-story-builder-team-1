/*
Allows for fast, organized prompt formatting. Templates for drafting, critiquing and judging stories.
*/

function judge_stories(story_bank, prompt_info) {
    //creates string from storybank object clearly delineating each story from each other
    var story_string = story_bank.map(entry => `Story index number ${entry.story_index}: ${entry.story}`).join("\n\n");
    var judge = {
        model: "llama3.1-8b", // Use model names from API documentation for model provider
        messages: [
            { "role": "system", "content": `Your job now is to judge all of these stories as objectively as you can based on the initial prompt information. You will choose the best story. Do not return an explanation for your decision and don't return the stories themselves, just return the story's index number. It's extremely important that you ONLY return the index number of the story you prefer and absolutely nothing else.` },
            { "role": "user", "content": `${prompt_info}\n\n${story_string}` },
        ],
        stream: false, 
    };
    return judge;
}

function judge_outlines(outline_bank, prompt_info) {
    //creates string from storybank object clearly delineating each story from each other
    var outline_string = outline_bank.map(entry => `Outline index number ${entry.outline_index}: ${entry.outline}`).join("\n\n");
    var judge = {
        model: "llama3.1-8b", // Use model names from API documentation for model provider
        messages: [
            { "role": "system", "content": `Your job is to judge the outlines sent to you and decide on the best one. These outlines will be used to generate stories, with LLMs generating entire chapters based on each 1-2 sentence synopsis you see corresponding to each chapter. You will choose the best outline, and you will make your decision based on the logical continuity of the outline as well as its originality and artistic merit. Do not return an explanation for your decision and don't return the outlines themselves, just return the outline's index number. It's extremely important that you ONLY return the index number of the outline you prefer and absolutely notihng else.` },
            { "role": "user", "content": `This is the prompt you will use to judge the outlines:${prompt_info}\n\nAnd these are the outlines:\n${outline_string}` },
        ],
        stream: false, 
    };
    return judge;
}

function judge_chapters(chapter_bank, prompt_info, previous_chapters) {
    //creates string from storybank object clearly delineating each story from each other
    var chapter_string = chapter_bank.map(entry => `Chapter index number ${entry.chapter_index}: ${entry.chapter}`).join("\n\n");
    var judge = {
        model: "llama3.1-8b", // Use model names from API documentation for model provider
        messages: [
            { "role": "system", "content": `Your job now is to judge all of these chapters as objectively as you can based on the initial prompt information. You will choose the best chapter based on its grammatical correctness and artistic merit, in addition to how well it fits with the previous chapters in the story. Do not return an explanation for your decision and don't return the stories themselves, just return the story's index number. It's extremely important that you ONLY return the index number of the chapter you prefer and absolutely notihng else.` },
            { "role": "user", "content": `The original prompt:\n${prompt_info}\n\nPrevious chapters in the story you will compare these chapters to: ${previous_chapters}\n\nAll chapters you will judge:\n\n${chapter_string}` },
        ],
        stream: false, 
    };
    return judge;
}

/*
This is the function used to call the first chapter in the story. Since we don't want to pass it any context (previous chapters), 
we will ONLY pass it the initial prompt and the outline generated by storyoutline. 
*/
function first_chapter(prompt_info, outline) {
    var draft = {
        model: "llama3.1-8b", // Use model names from API documentation for model provider
        messages: [
            { "role": "system", "content": `You are a helpful assistant. You will work in a Mechanical Turks style with other assistants to compose stories for users following a certain set of steps. The story will be written in chapters, and you will write the first chapter. It's very important that you don't return anything except for the chapter itself.`},
            { "role": "user", "content": `This is the prompt: ${prompt_info}\nThis is the story outline: ${outline}. \n\nYou will write the first chapter based on the chapter 1 summary above.`},
        ],
        stream: false, // Ensures a single response instead of a streamed response
    };
    return draft;
    }
    
/*
This function will generate a critique of any chapter sent to it. It will NOT rewrite the chapter, it will only generate strategies for it 
to be rewritten. It takes in the initial prompt, the chapter that is being critiqued, and the full story outline.
*/
function critique_chapter(prompt_info, chapter, outline) {
    var crit =  {
        model: "llama3.1-8b", 
        messages: [
            { "role": "system", "content": `You are now being fed a chapter written by another agent. You will critique the drafting of this chapter based on grammatical correctness as well as its faithfulness to the style parameters that were specified, and the previous chapters. Do not rewrite the chapter. It's very important that you ONLY return the critique and nothing else.`},
            { "role": "user", "content":  `${prompt_info}: Synopsis: ${outline}\n\nChapter: ${chapter}`},
        ],
        stream: false, 
    };
    return crit;
    }

function critique_outline(prompt_info, chapter, outline) {
    var crit =  {
        model: "llama3.1-8b", 
        messages: [
            { "role": "system", "content": `You are now being fed an outline written by another agent. You will critique this outline based on grammatical correctness as well as its faithfulness to the style parameters that were specified. Do not rewrite the outline. It is very important that you ONLY return the critique of the outline and nothing else.`},
            { "role": "user", "content":  `${prompt_info}: Synopsis: ${outline}`},
        ],
        stream: false, 
    };
    return crit;
    }

/*
This function rewrites the chapter based on the critique generated by critique(). You MUST call critique before calling rewrite.
This takes in the initial prompt, the chapter context, the critique generated by critique(), and the full story outline.
*/
function rewrite(prompt_info, chapter, critique, outline) {
        var rewrite =  {
            model: "llama3.1-8b", 
            messages: [
                { "role": "system", "content": `You are now being fed a chapter written by another agent, along with a critique of that chapter and the original prompt information. Rewrite the chapter, improving it based upon the critique's observations. Make sure it's a similar chapter length, and do not change anything if it doesn't violate the critique parameters. It's very important that you just return the rewritten chapter and nothing else.`},
                { "role": "user", "content":  `This is the prompt: ${prompt_info}\n\nThis is the critique: ${critique}\n\nThis is the outline of the entire story: ${outline}\n\nAnd this is the chapter: ${chapter}`},
            ],
            stream: false, 
        };
        return rewrite;
        }


/*
This function writes the next chapter in the story (any chapter that isn't chapter 1), because those chapters need context. 
It takes in the initial prompt, full story outline, the previous chapter (minimum, potentially ALL previous chapters),
and the number of the chapter that was previously written, which it will increment in the API request.
*/
function next_chapter(prompt_info, outline, chapter, chapter_count) {
    var chap =  {
        model: "llama3.1-8b", 
        messages: [
            { "role": "system", "content": `You are now being fed a chapter written by another agent. You will continue the story in another chapter of roughly equal length while still following the guidelines established in the original prompt.`},
            { "role": "user", "content": `${prompt_info}\n\nPrevious chapter(s) whose story you're continuing: ${chapter}\n\nStory outline: ${outline}\n\n Write chapter ${chapter_count+1}.`},
        ],
        stream: false, 
    };
    return chap;
}

function regenerate(prompt_info, outline, chapter, chapter_count) {
    var chap =  {
        model: "llama3.1-8b", 
        messages: [
            { "role": "system", "content": `You are now being fed a chapter written by another agent. You will regenerate chapter, keeping the length roughly equal length still following the guidelines established in the original prompt.`},
            { "role": "user", "content": `${prompt_info}\n\nAll previous chapters. The chapter you're regenerating is chapter ${chapter_count}: ${chapter}\n\nStory outline: ${outline}\n\n`},
        ],
        stream: false, 
    };
    return chap;
}

/*
This function returns a full outline of the story before it's written, with each chapter receieving a 1-2 sentence synopsis.
This will maintain consistency throughout the story writing process to ensure that chapters don't have redundant information 
and to ensure they don't contradict each other. It will be referenced throughout the process.
*/
function story_outline(chapter_count, prompt_info) {
    //create list to be filled by an AI with summaries of each chapter
    var outline_list = [];
    for(let i = 1; i <= chapter_count; i++) {
        //every chapter entry in the list has a (Fill this entry) stuck in its outline field so the LLM knows EXACTLY where to place its outline
        outline_list.push({chapter: i, outline: "(Fill this entry)"})
    }

    //create outline template for other templates to easily parse after the LLM call
    var outline_string = outline_list.map(entry => `Chapter ${entry.chapter}: ${entry.outline}`).join("\n");
    var outline_prompt =  {
        model: "llama3.1-8b", 
        messages: [
            { "role": "system", "content": `The following is a prompt for a story that will be split into ${chapter_count} chapters. You will write an outline for each chapter, no more than two sentences for each chapter. You will fill the provided list with a summary of each chapter.`},
            { "role": "user", "content": `${prompt_info}:\n\n ${outline_string}`},
        ],
        stream: false, 
    };
    return outline_prompt;
}

//Boosts model to a newer, higher-parameter version.
function boost_model(prompt) {
    prompt.model = "llama3.1-70b";
    return prompt;
}

//Downgrades model to a less expensive, lower-parameter version.
function degrade_model(prompt) {
    prompt.model = "llama3.1-1b";
    return prompt;
}

//Brings model back to the default 3.2 at 1B.
function default_model(prompt) {
    prompt.model = "llama3.1-8b";
    return prompt;
}

//Sets prompt settings to "stream" mode (this may be useful for real-time critiques)
function set_stream(prompt, stream_set) {
    if(stream_set == true) {
        prompt.stream = true;
    }
    else {
        prompt.stream = false;
    }
    return prompt;
}





module.exports = {judge_stories, judge_outlines, judge_chapters, first_chapter, critique_chapter, critique_outline, rewrite, next_chapter, regenerate, story_outline, boost_model, degrade_model, default_model, set_stream};
